\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{graphicx}
\textheight = 220mm \textwidth = 170mm \topmargin= -12mm
\oddsidemargin= -5mm \evensidemargin = -5mm \columnsep = 10mm
%\pagestyle{empty}
%\baselineskip 24 pt
\newlength{\mylen}
\newcommand{\ca}[3]{#1 \stackrel{#2}{\Longrightarrow} #3}
\newcommand{\cb}[3]{#1 \stackrel{#2^{\cal C}}{\Longrightarrow} #3}
\newcommand{\cc}[3]{#1 \stackrel{#2}{\longrightarrow} #3}
\newcommand{\mop}[2]{M,#1 \models^{+} #2}
\newcommand{\mon}[2]{M,#1 \models^{-} #2}
\newcommand{\moop}[2]{M,#1 \models_0^{+} #2}
\newcommand{\moon}[2]{M,#1 \models_0^{-} #2}
\newcommand{\moep}[2]{M,#1 \models_e^{+} #2}
\newcommand{\moen}[2]{M,#1 \models_e^{-} #2}
\newcommand {\epf}{\rule{.1in}{.1in}}
\newcommand {\ra}{\rightarrow}
\newcommand {\nmi}{\mid\!\sim}
\newcommand {\api}{\mid\!\approx}
\newcommand{\fracmy}[2]{\begin{array}{c} #1\\ \hline #2\end{array}}
\newcommand\mysert[4]{\begin{figure}[htp]\vspace{#3in}%
\setlength{\mylen}{\textwidth}\addtolength{\mylen}{-#2in}%
\setlength{\mylen}{0.5\mylen}%
\hspace*{\mylen}\special{picture #1}%
\caption{#4}\label{fig:#1}\end{figure}}
\newtheorem{myth}{Theorem}
\newtheorem{mypro}{Proposition}
\newtheorem{mylma}{Lemma}
\newtheorem{mydef}{Definition}
\newtheorem{mycor}{Corollary}
\newtheorem{myex}{Example}
\begin{document}
\bibliographystyle{plain}
\title{An Overview of Rough Set Theory from the Point of View of
Relational Databases}
\author{T.Y. Lin\\
tylin@cs.sjsu.edu\\
Berkeley Initiative in Soft Computing,\\
Department of Electrical Engineering and Computer Science,\\
University of California, Berkeley, California 94720}
\date{}

\maketitle

\noindent {\bf Article Outline}
\begin{description}
\item Glossary
\item Abstract 
\item 1. Introduction
\item 2. Information Tables
\item 3. Knowledge Dependencies
\item 3.1 Attributes and Equivalence relations
\item 3.2 Pawlak Knowledge bases
\item 3.3 Knowledge and Functional Dependencies
\item 4. Decision Tables and Decision Rules
\item 4.1 Reducts and Candidate Keys
\item 4.2 Decision Rules and Value Reducts
\item 4.3 Illustration
\item 5. Partial Dependencies
\item 6. Conclusions
\end{description}
\newpage

\section*{Glossary}
\begin{description}
\item [Binary Relation]: Binary relations are used in many branches of mathematics to model concepts like "is greater than", "is equal to", and "divides" in arithmetic, "is congruent to" in geometry, "is adjacent to" in graph theory, and many more. The all-important concept of function is defined as a special kind of binary relation
\item [Knowledge Dependencies]: A Knowledge is derivable from other knowledge, if all elementary categories of one  can be defined in terms of some elementary categories of other knowledge.
\item [Relational Databases]: A relational database is a database that conforms to the relational model, and refers to a database's data and schema (the database's structure of how that data is arranged). Common usage of the term "Relational database management system" technically refers to the software used to create a relational database, but sometimes mistakenly refers to a relational database
\item [Rough set]: A rough set is defined by the lower
and upper approximations of a concept. The lower approximation
contains all elements that necessarily belong to the concept, while
the upper approximation contains those that possibly belong  to the
concept. In rough set theory, a concept is considered a classical
set.
\item [Knowledge Base]: In general, a knowledge base is a centralized repository for information: a public library, a database of related information about a particular subject, and whatis.com could all be considered to be examples of knowledge bases. In relation to tnformation technology (IT), a knowledge base is a machine-readable resource for the dissemination of information, generally online or with the capacity to be put online. An integral component of knowledge management systems, a knowledge base is used to optimize information collection, organization, and retrieval for an organization, or for the general public.
\item [Consistent Table]: It is a table in which every decision rules are consistent that means each set of conditional attributes belong to unique decision attribute.
\item [Data Mining]: Data mining is the principle of sorting through large amounts of data and picking out relevant information. It is usually used by business intelligence organizations, and financial analysts, but it is increasingly used in the sciences to extract information from the enormous data sets generated by modern experimental and observational methods.
\item [Equivalence Relation]: Equivalence relation, a mathematical concept, is a type of relation on a given set that provides a way for elements of that set to be identified with (meaning considered equivalent to for some present purpose) other elements of that set. Equivalence relation is defined in a branch of mathematics called set theory, a vital branch underpinning all branches of mathematics and those fields that use mathematics. The power of an equivalence relation lies in its ability to partition a set into the disjoint union of subsets called equivalence classes.
\item [Fuzzy Sets]: Fuzzy sets are sets whose elements have degrees of membership. Fuzzy sets are an extension of the classical notion of set. In classical set theory, the membership of elements in a set is assessed in binary terms according to a bivalent condition - an element either belongs or does not belong to the set. By contrast, fuzzy set theory permits the gradual assessment of the membership of elements in a set; this is described with the aid of a membership function valued in the real unit interval [0, 1]. Fuzzy sets generalize classical sets, since the indicator functions of classical sets are special cases of the membership functions of fuzzy sets, if the latter only take values 0 or 1.
\item[Reduct]: a reduct is a subset of attributes that is jointly sufficient and individually necessary for preserving the same information or property as that is provided by the entire set of attributes.
\item[Indiscernible]: Two objects are considered to be indiscernible or equivalent if and only if they have the same values for all attributes in the set.
\end{description}
\newpage

\section*{Abstract} Both relational database theory (RDB) and
rough set theory (RS) are formal theories derived from the
study of tables. However, they were based on different
philosophies and went on to different directions. RDB  assumes
semantics of data is known and focuses on organizing data
through its semantics. RS, in this table format, assumes data
semantics is defined by the given data and focuses on
discovering patterns, rules and data semantics through those
available data - a data mining theory. In this paper,
fundamental notions of two theories are compared and
summarized. Further, RS can also take abstract format. In this
format it can be used to analyze imprecise, uncertain or
incomplete information in data. It is a new set theory
complimentary to fuzzy set theory.  In this paper, this aspect
is only lightly touched.
\vskip 10pt
\textbf{Keywords}: Rough sets, Relational Databases.
\section{Introduction}\label{sec1}
Rough set theory (RS)is a formal theory derived from
fundamental research on logical properties of information
tables, also known as (Pawlak) information systems or knowledge
representation systems, in Polish Academy of Sciences and the
University of Warsaw, Poland around 1970's. Developed
independently and differently from relational databases, RS in
the table format is another theory on extensional relational
databases (ERDB) - snap shots of relational databases. However
unlike usual ERDB  focusing on storing and retrieving data, RS
focuses on discovering patterns, rules and knowledge in data -
a modern data mining theory. Fundamentally RS and ERDB are very
different theory, even though the entities, namely tables, of
their respective studies are the same. In this paper we compare
and summarize their fundamental notions. Further RS in the
abstract format can be used to analyze imprecise, uncertain or
incomplete information in data - a new set theory complimentary
to fuzzy set theory. In this paper, this aspect is only
lightly touched.\\
\section{Information Table}\label{sec2}
The syntax of information tables in RS is very similar to
relations in RDB. Entities in RS are also represented by tuples
of attribute values. However, the representation may not be
faithful, namely, entities and tuples may not be one to one
correspondence.\\
\textit{A relation R} consists of
\begin{enumerate}
  \item U = {x,y....} is a set of entities.
  \item T is a set of attributes {A$_{1}$, A$_{2}$,...
      A$_{n}$}.
  \item Dom(A$_{i}$) is the set of values of attribute
      A$_{i}$..Dom = dom(A$_{1}$)$\bigcup$
      dom(A$_{2}$)$\bigcup$... $\bigcup$ dom(A$_{n}$),
  \item Each entity in U is represented uniquely by a map
		 t : T $\rightarrow$  Dom, where  t(A)
$\varepsilon$ dom(A$_{i}$)  for each A$_{i}$ $\varepsilon$
T.
\end{enumerate}
Informally, one can view relation as a table consists of rows
of elements. Each row represents an entity uniquely.

\textit{An information table} (also known as information
system,
knowledge representation system) consists of \\
\begin{enumerate}
  \item U = {u, v,..}is a set of entities.
  \item T is a set of attributes {A$_{1}$,
      A$_{2}$,...A$_{n}$}.
  \item Dom(A$_{i}$) is the set of values of attribute Ai.
	Dom = dom(A$_{1}$)* dom(A$_{2}$)* ...* dom(A$_{n}$),
  \item $\rho$ : U x T $\rightarrow$ Dom , called
      description function, is a 	map such that
	$\rho$(u, A$_{i}$) is in dom(A$_{i}$) for all u in U
and A$_{i}$ in T.
\end{enumerate}
Note that $\rho$ induces a set of maps\\
 t= $\rho$(u, $\bullet$) : T $\rightarrow$ Dom .\\
Each map is a tuple:  \\

t=($\rho$(u, A$_{1}$), $\rho$(u, A$_{2}$),....,$\rho$(u,
A$_{i}$),..$\rho$(u, A$_{n}$))

Note that the tuple t is not necessarily associated with entity
\textbf{uniquely}. In an information table, two distinct
entities could have the same tuple representation, which is
\textit{not permissible} in relational databases.\\
\textit{A decision table(DT}) is an information table (U, T, V,
$\rho$) in which the attribute set T = C $\bigcup$ D is a union
of two non-empty sets, C and D, of attributes. The elements in
C are called conditional attributes. The elements in D are
called decision attributes
\section{Knowledge Dependencies}\label{sec4}
Mathematically, a classifications or partition is a decomposition of the domain U of interest into mutually disjoint subsets, called equivalence classes. Such a partition defines and is defined by a binary relation, called an equivalence relation that is a reflexive, symmetric and transitive binary relation. In this section, we will compare the functional dependencies in RDB with Pawlak theory of equivalence relations derived from the structure of  information tables

\subsection{Attributes and Equivalence Relations}
In an information table, any subset of attributes induces an equivalence relation as follows: Let B be a non empty subset of T. Two entities u, v are equivalent (or indiscernible by B) in U, denoted by
\vskip 10pt
$u \cong v$ (mod B) if $\rho(u, A_i) = \rho(v, A_i)$ for every attribute $A_i \mbox{ in } B$
\vskip 10pt
It is not difficult to verify that $\cong$ is indeed an equivalence relation; $\cong$ is called indiscernibility relation and denoted by $IND(B)$. We will denote the equivalence class containing u by	$[u]_IND(B)$ or simply by $[u]_B$. Note that B can be a singleton $\{A_i\}$, in this case, we simply denoted by $IND(A_i)$.  It is easy to see that the following is valid:
\[IND(B) = \cap \{IND(A_i): A_i \mbox{ in } B\}\]

As observed earlier the equivalence classes of  IND(B) consists of all possible intersections of equivalence classes of $IND(A_i)$'s.\\
A set B of attributes names gives us a finite collection of equivalence relations,
\[RCol(B) = \{IND(A_i): A_i \mbox{ in } B\}\]
In particular, the set of all attributes give us the following set of equivalence relations,
\[RCol(T) = \{IND(A_1),IND(A_2),....,IND(A_i),...,IND(A_n)\}\]

\subsection{Pawlak Knowledge Bases}
Let RCol be a collection of equivalence relations, often a finite collection, over U.  An ordered pair
\[K = (U,RCol)\]
is called Pawlak knowledge base \cite{Pawlak91}. Pawlak calls a collection of equivalence relations a knowledge, because our knowledge about a domain is often represented by its classifications.\\ 
Let P and Q be two equivalence relations or classifications in K. If every Q-equivalence class is a union of P-equivalence classes, then we say that Q is depended on P, Q is coarse than P, or  P is finer than Q. The dependency is called \emph{knowledge dependency (KD)} \cite{Pawlak91}. It is easy to verify that the intersection $P \cap Q$ of two equivalence relations is another equivalence relation whose partition consists of all possible intersections of P- and Q-   equivalence classes. More generally, the intersection of all the equivalence relations in Rcol, denoted by IND(RCol), is another equivalence relation. IND(RCol) is referred to as the \emph{indiscernibility relation} over RCol. Let PCol and QCol be two sub-collections of RCol. Following Pawlak, we define the following \cite{Pawlak91}:
\begin{enumerate}
\item QCol depends on PCol iff IND(QCol) is coarse than IND(PCol).\\
This dependency is denoted by  $PCol \Rightarrow QCol$.
\item PCol and QCol are equivalent iff $PCol \Rightarrow QCol$, and $QCol \Rightarrow PCol$.\\
It is easy to see that PCol and QCol are equivalent iff IND(PCol)=IND(QCol).
\item PCol and QCol are independent iff neither $PCol \Rightarrow QCol$, nor $QCol \Rightarrow PCol$ 
\end{enumerate}
Now we can treat an information table as a Pawlak knowledge base (U, RCol(T)) and apply the notion of knowledge dependencies to information tables. Though Pawlak knowledge bases appear to be an abstract notion, it is in fact a very concrete object.\\

\textbf{Proposition.} There is an one-to-one correspondence between information tables and Pawlak knowledge bases.
\subsection{Knowledge and functional Dependencies}
In relational database, a finite collection of attribute names \[{ A_{1}, A_{2}, A_{2},....,A_{i},....,, A_{n} } \] is called a relational scheme \cite{Ulmann89}. A relation instance, or simply relation, R on relation scheme \underline{R} is a finite set of tuples, \[t=( t_{1}, t_{2},..., t_{n} ) \]as defined in Section 2. A functional dependency uniquely determine the value of another set of attributes. Formally, let X and Y be two subsets of T. A relation R satisfies an extensional function dependency EFD: $X \rightarrow Y$ if for very X-value there is a uniquely determined Y-value in the relation instance R. An intensional function dependency FD: $X \rightarrow Y$ exists on relation scheme $\underline{R}$, if FD satisfied by all relation instances R of the scheme $\underline{R}$. In database community, FD always refers to intensional FD. One  should note that at any given moment, a relation instance may satisfy some family of extension functional dependencies EFDs however, the same family may not be satisfied by other relation instances. The family that is satisfied by all the relation instances is the intensional functional dependency FD. In this paper, we will be interested in the extensional functional dependency, so the notation "$X \rightarrow Y$" is an EFD.
As pointed out earlier that attributes induce equivalence relations on information tables.  So an EFD can be interpreted as a knowledge  dependency $($KD$)$. The following proposition is immediate from the definitions.
Proposition.  An EFD,$ X \rightarrow Y$,  between two sets of attributes X and Y is equivalent to a KD, $RCol(X) \Rightarrow RCol(Y)$, of the equivalence relations induced by the attributes X and Y. 
\section{Decision Tables and Decision Rules}
Rough set theory is an effective methodology to extract rules from information tables, more precisely, from decision tables \cite{Pawlak91}. In this section we will introduce some fundamental concepts and illustrate the procedure by an example. 
\subsection{Reducts and Candidate Keys}
In RDB, an attribute, or a set of attributes K is called candidate key if all attributes is functionally depended on K, and K is such a minimal set. We export these notions to extensional world. The "extensional candidate key" is a special form of reduct \cite{Pawlak91}. 
Let S = $(U, T=C  D, V, \rho)$ be a decision table, where

\[C= {A_{1} , A_{2} ,...,  A_{i} ,...,   A_{n}}, \]
\[D= {B_{1} , B_{2} ,...,  B_{i} ,...,   B_{m}}.\]

Then there are two Pawlak knowledge bases on U:

\[RCol(C)={IND(A_{1}),IND(A_{2}),..., IND(A_{i}),...,  IND(A_{n})} \]
\[RCol(D)={IND(B_{1}),IND(B_{2}),..., IND(B_{i}),...,  IND(B_{m})}\]

S is a consistent decision table, if $RCol(C) \Rightarrow RCol(D)$. B is called a reduct of S, if  B is a minimal subset of C such that $RCol(B) \Rightarrow RCol(D)$. It is clear such a B is not necessarily unique.If we choose D=T, then the reduct is the extensional candidate key.

\subsection{Decision Rules and Value Reducts}
Rough set theory is an effective methodology to extract rules from information tables (IT), more precisely, from decision tables (DT); each IT can be viewed as many DT's. Each entity u in a DT can be interpreted as a decision rule.  Let $X \rightarrow Y$ be an EFD (or equivalently a KD), that is, for any X-value c, there is a unique Y-value d.  We can rephrase it as a decision rule:

If  \[t(X)=c, then t(Y)=d.\]

Rough set theorists are interested in simplified these decision rules \cite{Pawlak91}.

\subsection{Illustration}
We will illustrate, without losing the general idea from the following table:

\[
\begin{tabular}{|l|l|l|l|l|l|}
\hline
U&Location&TEST&NEW&CASE&RESULT\\
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
  ID-1  & Houston   & 10 & 92 & 03 & 10 \\ \hline
  ID-2  & San Jose  & 10 & 92 & 03 & 10 \\ \hline
  ID-3  & Palo Alto & 10 & 90 & 02 & 10 \\ \hline
  ID-4  & Berkeley  & 11 & 91 & 04 & 50 \\ \hline
  ID-5  & New York  & 11 & 91 & 04 & 50 \\ \hline
  ID-6  & Atlanta   & 20 & 93 & 70 & 99 \\ \hline
  ID-7  & Chicago   & 20 & 93 & 70 & 99 \\ \hline
  ID-8  & Baltimore & 20 & 93 & 70 & 99 \\ \hline
  ID-9  & Seattle   & 20 & 93 & 70 & 99 \\ \hline \hline
  ID-10 & Chicago  & 51 & 95 & 70 & 94 \\ \hline
  ID-11 & Chicago  & 51 & 95 & 70 & 95 \\ \hline
  \hline
\end{tabular} 
\]
Two rows below the bold double line will be cut off.

\subsubsection {}Select a DT: We will consider a decision table from the table given above: U is the universe, RESULT is the decision attribute, and C={TEST, NEW, CASE} is the set of conditional attributes.

\subsubsection {}Split DT: ID-10 and ID-10 form two inconsistent rules. So we split that tables. One consists of entities, ID-1 to ID-9, called consistent table, and another one is ID-10 and ID-11 is called totally inconsistent table. From now on the term in this section "decision table" is referred to the consistent table.
\subsubsection {} Decision Classes: Using notation of Section 3.1, the equivalence relation IND(RESULT) classifies entities into three equivalence classes, called decision classes 
\begin{center}
   DECISION1=\{ID-1,ID-2,ID-3\}=$[10]_{RESULT}$, \newline
   DECISION2=\{ID-4,ID-5\}=$[50]_{RESULT}$, \newline
   DECISION3=\{ID-6,ID-7,ID-8,ID-9\}=$[ 99]_{RESULT}$ \newline  
\end{center}

\subsubsection {}Condition Classes: Let C = $\{$TEST, NEW, CASE$\}$ be the conditional attributes. The equivalence relation IND(C) classifies entities into four equivalence classes, called condition classes

\begin{center}
CASE1=\{ID-1, ID-2\}, CASE2=\{ID-3\}, \newline
CASE3=\{ID-4, ID-5\}, CASE4=\{ID-6, ID-7,ID-8,ID-9\} \newline
\end{center}

\subsubsection {}Knowledge  dependencies: It is not difficult to verify that entities are indiscernible by conditional attributes are also indiscernible by decision attributes, namely we have following inclusions

\begin{center}
CASE1 $\subseteq$  DECISION1;     CASE2 $\subseteq$  DECISION1;\newline
CASE3 $\subseteq$  DECISION2;     CASE4 $\subseteq$  DECISION3;\newline
\end{center}

These inclusions implies that the equivalence relation IND (RESULT) depends on IND(C). Or equivalently, RESULT are KD on C.

\subsubsection {}Inference Rules: The relationship induces inference rules:
\begin{enumerate}
\item If  TEST=  10 , NEW= 92,  CASE= 03, then RESULT= 10,
\item If  TEST=  10 , NEW= 90,  CASE= 02, then RESULT= 10,
\item If  TEST=  11 , NEW= 91,  CASE= 04, then RESULT= 50,
\item If  TEST=  20 , NEW= 93,  CASE= 70, then RESULT= 99.
\end{enumerate}
One can rewrite these four rules into one universal inference rule " (TEST, NEW, CASE) implies RESULT."

\subsubsection {}Reducts: It is clear that  {TEST, NEW} and {Case} are two reducts. So the conditions on CASE can be deleted from the rules given above.

\subsubsection {}
Value Reducts: Note that the rule 1 is derived from the inclusion CASE1 DECISION1.  We can sharpen the description of CASE1 by the condition TEST=10 alone, called value reducts, i.e.,
\begin{center}
CASE1=$\{ u: u.TEST=10\}$
\end{center}
By similar arguments, we can simplify the four rules to:
\begin{enumerate}
\item If  TEST=  10 , then RESULT= 10,
\item If  TEST=  11 ,then RESULT= 50,
\item If  TEST=  20 ,then RESULT= 99.
\end{enumerate}
We will have another set of rules, if the other reduct is used.

\section{Partial Dependencies}
The table given above is assuming that data have no noise. In practices,  noise are unavoidable. So we will examine next the partial dependency.  As explained,  D and C may induce two distinct partitions of the universe.  Consider the problem of approximating equivalence classes of  IND(D) using the equivalence classes of IND(C).  For an equivalent class [X]D, the lower approximation is given by:

$C$\textunderscore $L (X)  = \{u \space | \space [u]_c \subset C \}  = \cup \{ [u]_c | [u]_c \subset C \}$\newline

\noindent The upper approximation of X is defined as,

$C \mbox{\textunderscore} H(X)  =  \{u \space | \space [u]_c \cap C \neq \phi \} = \cup \{ [u]_c | [u]_c  \cap C \neq \phi \}$ \newline

\noindent With the lower approximations of every equivalence class of D, the C-positive region of D, denoted by  POSC(D), is defined by:

$POS_{C(D)} = \cup \{ C \mbox{\textunderscore} L(X) : $X are all decision classes$\}$. \newline

\noindent The number 
$k = | POS_{C(D)} | / |U|$

is called degree of dependency of D on C, where $|$ " $|$ denotes the cardinality of a set.  If degree =1, then we have the knowledge dependency. By applying k-dependency, one can convert the previous example into an example of approximate rules or soft rules [Lin93], [Ziarko93], [Lin96c]. In RDB, there are no such a concept of partial dependencies.

\section{Conclusion}
In this paper, we use relational databases to explain some aspects of rough set theory.  Rough set theory, in its table form, has been an effective method in rule mining for small to median size data. The theory and methodology has been extended to very large databases \cite{Lin96b}. Some applications to intelligent control design have been examined \cite{Lin96a}, \cite{Lin96b}, \cite{Mrozek96}, \cite{Munakata96}. RS, in its abstract format, is a new set theory complimentary to fuzzy theory \cite{Zimmermann91}. Its approximation aspect is closely related to modal logic and (pre-)topological spaces (neighborhood systems) \cite{Chellas80},\cite{Lin88}. Rough set theory provides a new point of views to the theory of relation databases. We believe some profound applications in databases through this view may soon be flourished.

\bibliography{bibfile}

\noindent Tsau Young (T. Y.) Lin received his Ph.D from Yale University, and now is a
Professor at San Jose State University and Visiting Scholar at BISC, University of California-Berkeley.
He is the president of International Rough Set Society. He has been the chair and a member of the program
committee of various conferences and workshops. He is also serving as an associate editor and member
of editorial board of several international journals. He is interesting in approximate retrieval,
approximate reasoning, data mining, data security, fuzzy sets, intelligent control, Petri nets,
and rough sets (alphabetical order).
\end{document}
