\subsection{Implementation}
\label{sec:implementation}

% INTRO
In order to evaluate our approach outlined in section \ref{sec:approx}
  we implemented an algorithm that converts RDF data
  containing identity and/or alignment statements
  to a hierarchy describing its identity sub-relations,
  its lower, and its higher approximation.

% MATERIALIZATION
For materializing the graph under RDFS and OWL entailment,
  we use Jena 2.11.0 \cite{Carroll2004}.

% ALIGNMENT
Our code also supports the loading of alignment files that are formatted in
  the Expressive and Declarative Ontology Alignment Language (EDOAL)
  format \cite{DavidEzenatScharffeTrojahn2011}.
This allows OAEI datasets to be loaded easily.

% AJAX, PRECISION
Interactive Ajax code allows the user to click on nodes in the SVG graphic
  to navigate to descriptions of the resource pairs that occur in
  that partition set while not being in the identity relation.
This implementation may facilitate the validation of hypotheses in this
  new experimental setup.

% PROLOG
The code is written in SWI-Prolog 7.1.1 \cite{Wielemaker2003}
  and is deployed as an extension pack
  of the ClioPatria triple store \cite{Schreiber2006}.
% GRAPHVIZ DOT
For visualizing the results we use GraphViz 2.30.1 Dot.
% SVG
For ease of use the visualizations are displayed in a Web interface
  using SVG for the sub-relation hierarchy.

% COMPLEXITY: EASY
Calculating the identity sub-relations, i.e. the higher approximation,
  does not require a partitioning of the cartesian product of the domain.
Since in real-world data the number of identity pairs
  is relatively small when compared to the number of possible pairs,
  this is not expensive to calculate.
Even the generialized predicates extension
  from section \ref{sec:generalized_predicates} is quite efficient,
  since the number of edge types in RDF data is realtively big
  (so there are not many sequences of identical predicates).

It is also quite efficient to calculate the lower approximation,
  since there are relatively few sub-relations
  that belong to the lower approximation,
  and most such sub-relations are relatively small.

% COMPLEXITY: DIFFICULT
It is more costly to calculate the number of the higher approximation pairs
  that are not in the lower approximation,
  since this requires an inverse search
  starting from sets of indiscernibility predicates
  and resulting in (non-identity) pairs.
These number are, however, necessary in order to
  calculate the quality of the identity relation
  as well as the precision (definition \ref{def:precision})
  of each sub-relation.

Since the quality and sub-relation precision metrics are
  useful for applications (see section \ref{sec:applications}),
  we want to optimize these operations as well.
For this we have implemented the following three optimizations.

% (1) QUERY OPTIMIZATION
Firstly, we optimize the search by restricting the number of pairs
  for which we have to check whether they share a given set of predicates.
We do this by ordering the predicates based on their
  estimated complexity \cite{Wielemaker2005},
  allowing us to match triples containing rare predicates before
  matching frequent predicates.
Additionally, by using a strict order on terms (e.g. lexicographically),
  only half the search space needs to be taken into account
  due to the symmetric property of equivalence.

% (2) DATASCRUTURES
Secondly, we use an AVL tree-based association list
  with ordered sets as values,
  in order to store the indiscernibility predicates.
The storage of indiscernibility properties uses a similar, but nested
  association list (mapping sets of predicate terms onto
  maps from sets of object terms onto sets of subject term pairs).
This means that most operations on datastructures are at most $O(\log(n))$.

% (3) XML DATATYPES: CANONICAL FORM
Thridly, it is expensive to determine whether lexical expressions
  of the same datatype are identical or not
  due to the time spent on parsing those expressions
  to determine their values.
The number of parses is quadratic in the search for pairs that share
  a given set of predicates.
This is resolved by converting the lexical expressions
  of all typed literals in the dataset to their canonical form,
  prior to running the main algorithm.
Since canonical mappings are one-to-one \cite{XmlSchema2012},
  it is possible to determine value identity based on
  canonical lexical expression comparison,
  requiring only a linear number of lexical expression parses.

% XML DATATYPES
Existing SW libraries implement equivalence relations for common datatypes.
Equivalence is not always in line with identity though.
We have made exceptions for specific values that are known to have
  equivalent non-identities (such as $0$ and $-0$)
  non-equivalent identities (such as $NaN$ for \texttt{float}) \cite{XSD11}.
The latter even provides a rare instance of the violation of
  definition \ref{def:identity}.

